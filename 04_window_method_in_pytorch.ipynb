{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/Transformers/blob/main/04_window_method_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhxlq49luPwo"
      },
      "source": [
        "# Window Method in PyTorch\n",
        "\n",
        "In the previous section we built a method for calculating the average sentiment for long pieces of text by breaking the text up into *windows* and calculating the sentiment for each window individually.\n",
        "\n",
        "Our approach in the last section was a quick-and-dirty solution. Here, we will work on improving this process and implementing it solely using PyTorch functions to improve efficiency.\n",
        "\n",
        "The first thing we will do is import modules and initialize our model and tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsYJGtTbuPwr"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "model = BertForSequenceClassification.from_pretrained('ProsusAI/finbert')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmyv-P1muPwt"
      },
      "source": [
        "We will be using the same text example as we did previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLpXxa8QuPwu"
      },
      "outputs": [],
      "source": [
        "txt = \"\"\"\n",
        "I would like to get your all  thoughts on the bond yield increase this week.  I am not worried about the market downturn but the sudden increase in yields. On 2/16 the 10 year bonds yields increased by almost  9 percent and on 2/19 the yield increased by almost 5 percent.\n",
        "\n",
        "Key Points from the CNBC Article:\n",
        "\n",
        "* **The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.**\n",
        "* **Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic.**\n",
        "* **However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.**\n",
        "\n",
        "The recent rise in bond yields and U.S. inflation expectations has some investors wary that a repeat of the 2013 “taper tantrum” could be on the horizon.\n",
        "\n",
        "The benchmark U.S. 10-year Treasury note climbed above 1.3% for the first time since February 2020 earlier this week, while the 30-year bond also hit its highest level for a year. Yields move inversely to bond prices.\n",
        "\n",
        "Yields tend to rise in lockstep with inflation expectations, which have reached their highest levels in a decade in the U.S., powered by increased prospects of a large fiscal stimulus package, progress on vaccine rollouts and pent-up consumer demand.\n",
        "\n",
        "The “taper tantrum” in 2013 was a sudden spike in Treasury yields due to market panic after the Federal Reserve announced that it would begin tapering its quantitative easing program.\n",
        "\n",
        "Major central banks around the world have cut interest rates to historic lows and launched unprecedented quantities of asset purchases in a bid to shore up the economy throughout the pandemic. The Fed and others have maintained supportive tones in recent policy meetings, vowing to keep financial conditions loose as the global economy looks to emerge from the Covid-19 pandemic.\n",
        "\n",
        "However, the recent rise in yields suggests that some investors are starting to anticipate a tightening of policy sooner than anticipated to accommodate a potential rise in inflation.\n",
        "\n",
        "With central bank support removed, bonds usually fall in price which sends yields higher. This can also spill over into stock markets as higher interest rates means more debt servicing for firms, causing traders to reassess the investing environment.\n",
        "\n",
        "“The supportive stance from policymakers will likely remain in place until the vaccines have paved a way to some return to normality,” said Shane Balkham, chief investment officer at Beaufort Investment, in a research note this week.\n",
        "\n",
        "“However, there will be a risk of another ‘taper tantrum’ similar to the one we witnessed in 2013, and this is our main focus for 2021,” Balkham projected, should policymakers begin to unwind this stimulus.\n",
        "\n",
        "Long-term bond yields in Japan and Europe followed U.S. Treasurys higher toward the end of the week as bondholders shifted their portfolios.\n",
        "\n",
        "“The fear is that these assets are priced to perfection when the ECB and Fed might eventually taper,” said Sebastien Galy, senior macro strategist at Nordea Asset Management, in a research note entitled “Little taper tantrum.”\n",
        "\n",
        "“The odds of tapering are helped in the United States by better retail sales after four months of disappointment and the expectation of large issuance from the $1.9 trillion fiscal package.”\n",
        "\n",
        "Galy suggested the Fed would likely extend the duration on its asset purchases, moderating the upward momentum in inflation.\n",
        "\n",
        "“Equity markets have reacted negatively to higher yield as it offers an alternative to the dividend yield and a higher discount to long-term cash flows, making them focus more on medium-term growth such as cyclicals” he said. Cyclicals are stocks whose performance tends to align with economic cycles.\n",
        "\n",
        "Galy expects this process to be more marked in the second half of the year when economic growth picks up, increasing the potential for tapering.\n",
        "\n",
        "## Tapering in the U.S., but not Europe\n",
        "\n",
        "Allianz CEO Oliver Bäte told CNBC on Friday that there was a geographical divergence in how the German insurer is thinking about the prospect of interest rate hikes.\n",
        "\n",
        "“One is Europe, where we continue to have financial repression, where the ECB continues to buy up to the max in order to minimize spreads between the north and the south — the strong balance sheets and the weak ones — and at some point somebody will have to pay the price for that, but in the short term I don’t see any spike in interest rates,” Bäte said, adding that the situation is different stateside.\n",
        "\n",
        "“Because of the massive programs that have happened, the stimulus that is happening, the dollar being the world’s reserve currency, there is clearly a trend to stoke inflation and it is going to come. Again, I don’t know when and how, but the interest rates have been steepening and they should be steepening further.”\n",
        "\n",
        "## Rising yields a ‘normal feature’\n",
        "\n",
        "However, not all analysts are convinced that the rise in bond yields is material for markets. In a note Friday, Barclays Head of European Equity Strategy Emmanuel Cau suggested that rising bond yields were overdue, as they had been lagging the improving macroeconomic outlook for the second half of 2021, and said they were a “normal feature” of economic recovery.\n",
        "\n",
        "“With the key drivers of inflation pointing up, the prospect of even more fiscal stimulus in the U.S. and pent up demand propelled by high excess savings, it seems right for bond yields to catch-up with other more advanced reflation trades,” Cau said, adding that central banks remain “firmly on hold” given the balance of risks.\n",
        "\n",
        "He argued that the steepening yield curve is “typical at the early stages of the cycle,” and that so long as vaccine rollouts are successful, growth continues to tick upward and central banks remain cautious, reflationary moves across asset classes look “justified” and equities should be able to withstand higher rates.\n",
        "\n",
        "“Of course, after the strong move of the last few weeks, equities could mark a pause as many sectors that have rallied with yields look overbought, like commodities and banks,” Cau said.\n",
        "\n",
        "“But at this stage, we think rising yields are more a confirmation of the equity bull market than a threat, so dips should continue to be bought.”\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ia_KVTtuPww"
      },
      "source": [
        "This time, because we are using PyTorch, we will specify `return_tensors='pt'` when encoding our input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMbGy9oOuPwx",
        "outputId": "f189fd73-527d-43cd-8aea-cfdbf134eef4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1345 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1345\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[1045, 2052, 2066,  ..., 4149, 1012, 1524]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens = tokenizer.encode_plus(txt, add_special_tokens=False,\n",
        "                               return_tensors='pt')\n",
        "\n",
        "print(len(tokens['input_ids'][0]))\n",
        "tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizes a given text using a pre-trained financial tokenizer\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Import necessary libraries:\n",
        "\n",
        "    from transformers import BertTokenizer\n",
        "\n",
        "This imports the BertTokenizer class from the transformers library.\n",
        "\n",
        "-------------------------\n",
        "##Initialize the tokenizer:\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('ProsusAI/finbert')\n",
        "\n",
        "This line initializes a tokenizer specifically designed for financial text, using the pre-trained model 'ProsusAI/finbert'.\n",
        "\n",
        "------------------------\n",
        "##Tokenize the text:\n",
        "\n",
        "    tokens = tokenizer.encode_plus(txt, add_special_tokens=False, return_tensors='pt')\n",
        "\n",
        "This line tokenizes the input text txt using the initialized tokenizer.\n",
        "\n",
        "    add_special_tokens=False\n",
        "    \n",
        "Prevents the automatic addition of special tokens like [CLS] and [SEP].\n",
        "\n",
        "In the provided code, the text is being processed in a specific way for sentiment analysis using a windowing method.\n",
        "\n",
        "The code aims to split the text into smaller chunks (windows) of a specific size (512 tokens).\n",
        "\n",
        "###Here's why add_special_tokens=False is important:\n",
        "\n",
        "###Manual Control:\n",
        "\n",
        "By setting add_special_tokens=False, the code avoids the tokenizer automatically adding special tokens like [CLS] and [SEP] at the beginning and end of the entire text.\n",
        "\n",
        "This gives the code more control over where these tokens are placed.\n",
        "\n",
        "###Windowing Approach:\n",
        "\n",
        "The code later manually inserts the [CLS] and [SEP] tokens at the beginning and end of each individual chunk (window) of text.\n",
        "\n",
        "This ensures each chunk is treated as a separate sequence for sentiment analysis.\n",
        "\n",
        "If the special tokens were added at the beginning and end of the entire text, the windowing approach wouldn't work correctly.\n",
        "\n",
        "###Padding and Chunk Size:\n",
        "\n",
        "The code carefully calculates the required padding to ensure each chunk is exactly 512 tokens long, including the manually added [CLS] and [SEP] tokens.\n",
        "\n",
        " This consistent chunk size is important for efficient processing by the model. If special tokens were automatically added, the chunk sizes would vary, potentially exceeding the desired length.\n",
        "\n",
        "###In summary,\n",
        "\n",
        "setting add_special_tokens=False provides more control over token placement, which is crucial for the windowing approach used in this code to process long text for sentiment analysis with specific chunk sizes.\n",
        "\n",
        "By manually adding the special tokens to each chunk, the code ensures that the chunks are formatted correctly for the model and padded to the desired length.\n",
        "\n",
        "    return_tensors='pt'\n",
        "\n",
        "Ensures the output is in PyTorch tensor format.\n",
        "\n",
        "-------------------------------\n",
        "##Print the length of the tokenized input:\n",
        "\n",
        "    print(len(tokens['input_ids'][0]))\n",
        "\n",
        "This line prints the length of the tokenized input, accessed via tokens['input_ids'][0], which represents the numerical IDs of the tokens.\n",
        "\n",
        "-------------------------\n",
        "##Display the tokenized output:\n",
        "\n",
        "    tokens\n",
        "\n",
        "This line displays the full tokenized output, including input IDs and attention mask.\n",
        "\n",
        "-----------------------------\n",
        "##In essence,\n",
        "\n",
        "this code snippet tokenizes a given text using a pre-trained financial tokenizer and displays the length and content of the tokenized output as PyTorch tensors."
      ],
      "metadata": {
        "id": "zsOKoT0DvHsB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSHAWZvGuPwy"
      },
      "source": [
        "Now we have a set of tensors where each tensor contains **1345** tokens. We will use a similiar approach to what we used before where we will pull out a length of **510** tokens (or less), add the CLS and SEP tokens, then add PAD tokens when needed. To create these tensors of length **510** we  need to use the `torch.split` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw9XtC3zuPwz",
        "outputId": "a91a6c44-e521-472f-8c37-108b4d942fb6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.arange(10)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tr_b5Hbmz2Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creates a 1-D tensor\n",
        "This line creates a 1-D tensor named a using the\n",
        "\n",
        "    torch.arange function. torch.arange(10)\n",
        "\n",
        "generates a sequence of numbers from 0 to 9 (exclusive) and creates a tensor from it.\n",
        "\n",
        "--------\n",
        "##Display the tensor:\n",
        "\n",
        "    a\n",
        "\n",
        "This line displays the content of the tensor a.\n",
        "\n",
        "To see the output, run the code. You will see a tensor containing the numbers 0 to 9.\n",
        "\n",
        "----------------\n",
        "##In essence,\n",
        "\n",
        "this code snippet creates a simple 1-D tensor containing a sequence of numbers and displays it. This is a fundamental operation in PyTorch for creating and working with tensors."
      ],
      "metadata": {
        "id": "58OVxPo5zreR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8GbTf2DuPwz",
        "outputId": "e0e954cb-ec66-44fc-8e4f-1b30161068c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0, 1, 2, 3]), tensor([4, 5, 6, 7]), tensor([8, 9]))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.split(a, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP8VlN37uPw0"
      },
      "source": [
        "Now we apply `split` to our *input IDs* and *attention mask* tensors. Note that we must access the first element of each tensor because they are shaped like a list within a list (you can see this by comparing the number of square brackets between tensor `a` above, and the tensors shown when outputting `tokens` above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuwwYfzKuPw1"
      },
      "outputs": [],
      "source": [
        "input_id_chunks = tokens['input_ids'][0].split(510)\n",
        "mask_chunks = tokens['attention_mask'][0].split(510)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting long text into chunks\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "     tokens['input_ids'][0]:\n",
        "\n",
        "This part accesses the input IDs from the tokens dictionary.\n",
        "\n",
        "tokens likely holds the output of a tokenization process, where text is converted into numerical representations.\n",
        "\n",
        "\n",
        "      ['input_ids']\n",
        "\n",
        "retrieves the specific key containing the input IDs.\n",
        "\n",
        "      [0]\n",
        "\n",
        "selects the first element of the input IDs, which is likely a tensor or a list representing the tokenized text.\n",
        "\n",
        "--------------------------\n",
        "    .split(510):\n",
        "\n",
        "This applies the split method to the selected input IDs.\n",
        "\n",
        "split(510) divides the input IDs into chunks of 510 tokens each.\n",
        "\n",
        "If the total number of tokens is not perfectly divisible by 510, the last chunk will have fewer tokens.\n",
        "\n",
        "    input_id_chunks = ...:\n",
        "\n",
        "This assigns the resulting chunks to the variable input_id_chunks.\n",
        "\n",
        "In essence, this line splits the tokenized input IDs into smaller chunks of 510 tokens each and stores them in input_id_chunks.\n",
        "\n",
        "-----------------------------\n",
        "Line 2: mask_chunks = tokens\n",
        "\n",
        "    ['attention_mask'][0].split(510)\n",
        "\n",
        "This line follows the same logic as the first, but instead of input IDs, it works with the attention mask.\n",
        "\n",
        "     tokens['attention_mask'][0]:\n",
        "\n",
        "This selects the attention mask from the tokens dictionary and takes its first element.\n",
        "\n",
        "    .split(510):\n",
        "\n",
        "Similar to before, this splits the attention mask into chunks of 510 elements.\n",
        "\n",
        "    mask_chunks = ...:\n",
        "\n",
        "The resulting chunks are stored in the variable mask_chunks.\n",
        "\n",
        "This line splits the attention mask into chunks of 510 elements each, aligning with the input ID chunks, and assigns them to mask_chunks.\n",
        "\n",
        "-------------------------------\n",
        "#Reasoning:\n",
        "\n",
        "This splitting into chunks is often used when dealing with long sequences in natural language processing.\n",
        "\n",
        "It allows you to process the text in smaller, more manageable pieces, which can be important for computational efficiency or when working with models that have limitations on input sequence length.\n",
        "\n",
        "The attention mask is used to indicate which tokens should be attended to by the model, so splitting it alongside the input IDs ensures that the attention mechanism works correctly on each chunk.\n",
        "\n",
        "The line torch.split(a, 4) serves as an introductory example of the split functionality. The actual application of this concept occurs later in the code, utilizing the .split(510) method on the input IDs and attention mask tensors."
      ],
      "metadata": {
        "id": "l7hfNvGb1sSw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd2-tqk4uPw1"
      },
      "source": [
        "# Add Start & Finish tokens in the splits through tensor concatenation\n",
        "To add our CLS (**101**) and SEP (**102**) tokens, we can use the `torch.cat` method. This method takes a *list* of tensors and con**cat**enates them. Let's try it on our example tensor `a` first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aLSV9mJuPw1",
        "outputId": "7f7ee457-50c5-42b2-aa82-34efd030ab21"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([101.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9., 102.])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.cat(\n",
        "    [torch.Tensor([101]), a, torch.Tensor([102])]\n",
        ")\n",
        "\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evyPeGR7uPw2"
      },
      "source": [
        "# Padding the Chunks\n",
        "It's that easy! We're almost there now, but we still need to add padding to our tensors to push them upto a length of *512*, which should only be required for the final chunk.\n",
        "\n",
        "To do this we will build an if-statement that checks if the tensor length requires padding, and if so add the correct amount of padding which will be something like `required_len = 512 - tensor_len`.\n",
        "\n",
        "Again, let's test it on tensor `a` first:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gphpe065uPw2",
        "outputId": "7cb8b68d-467e-40e3-bb65-2e508bdc2257"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padding_len = 20 - a.shape[0]\n",
        "\n",
        "padding_len"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding Calculated\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Line 1:\n",
        "\n",
        "    padding_len = 20 - a.shape[0]\n",
        "\n",
        "    a.shape[0]:\n",
        "\n",
        "This retrieves the size of the first dimension of the tensor a.\n",
        "\n",
        "In this case, since a is a 1-dimensional tensor, a.shape[0] represents the total number of elements in the tensor.\n",
        "\n",
        "    20 - a.shape[0]:\n",
        "\n",
        "This subtracts the size of the tensor a from 20. The result represents the number of padding elements that would be needed to make the tensor have a length of 20.\n",
        "\n",
        "    padding_len = ...:\n",
        "\n",
        "This assigns the calculated padding length to the variable padding_len.\n",
        "\n",
        "\n",
        "Line 2:\n",
        "\n",
        "    padding_len\n",
        "\n",
        "This line simply displays the value of padding_len.\n",
        "\n",
        "If the tensor a has fewer than 20 elements, padding_len will be a positive number indicating the amount of padding needed.\n",
        "\n",
        "If the tensor a already has 20 or more elements, padding_len will be 0 or a negative number, indicating that no padding is required.\n",
        "\n",
        "In essence, these lines calculate the amount of padding needed to bring the length of the tensor a up to 20 and store this value in padding_len.\n",
        "\n",
        "----------------------------\n",
        "##Example:\n",
        "\n",
        "If a has 15 elements,\n",
        "    a.shape[0]\n",
        "\n",
        "would be 15.\n",
        "\n",
        "Then, padding_len would be calculated as 20 - 15 = 5.\n",
        "\n",
        "This means 5 padding elements would be needed to make a have a length of 20.\n",
        "\n",
        "This is just an illustrative example."
      ],
      "metadata": {
        "id": "J2_ypkN9A4XL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EtNfkxI_CJOm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q77NcDb2uPw3",
        "outputId": "32b122cf-43b6-4f03-a61c-6a5cd779d91b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([101.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9., 102.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if padding_len > 0:\n",
        "    a = torch.cat(\n",
        "        [a, torch.Tensor([0] * padding_len)]\n",
        "    )\n",
        "\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isi2OoLGuPw4"
      },
      "source": [
        "Now let's use the same logic with our `tokens` tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjkvyKuSuPw4",
        "outputId": "df4ce79e-3f58-461e-a8fb-f39d370a64e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "512\n",
            "512\n",
            "512\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([  101.,  2153.,  1010.,  1045.,  2123.,  1521.,  1056.,  2113.,  2043.,\n",
              "         1998.,  2129.,  1010.,  2021.,  1996.,  3037.,  6165.,  2031.,  2042.,\n",
              "         9561.,  7406.,  1998.,  2027.,  2323.,  2022.,  9561.,  7406.,  2582.,\n",
              "         1012.,  1524.,  1001.,  1001.,  4803., 16189.,  1037.,  1520.,  3671.,\n",
              "         3444.,  1521.,  2174.,  1010.,  2025.,  2035., 18288.,  2024.,  6427.,\n",
              "         2008.,  1996.,  4125.,  1999.,  5416., 16189.,  2003.,  3430.,  2005.,\n",
              "         6089.,  1012.,  1999.,  1037.,  3602.,  5958.,  1010., 23724.,  2015.,\n",
              "         2132.,  1997.,  2647., 10067.,  5656., 14459.,  6187.,  2226.,  4081.,\n",
              "         2008.,  4803.,  5416., 16189.,  2020.,  2058., 20041.,  1010.,  2004.,\n",
              "         2027.,  2018.,  2042.,  2474., 12588.,  1996.,  9229., 26632., 23035.,\n",
              "        17680.,  2005.,  1996.,  2117.,  2431.,  1997., 25682.,  1010.,  1998.,\n",
              "         2056.,  2027.,  2020.,  1037.,  1523.,  3671.,  3444.,  1524.,  1997.,\n",
              "         3171.,  7233.,  1012.,  1523.,  2007.,  1996.,  3145.,  6853.,  1997.,\n",
              "        14200.,  7302.,  2039.,  1010.,  1996.,  9824.,  1997.,  2130.,  2062.,\n",
              "        10807., 19220.,  1999.,  1996.,  1057.,  1012.,  1055.,  1012.,  1998.,\n",
              "         7279.,  2102.,  2039.,  5157., 15801.,  2011.,  2152.,  9987., 10995.,\n",
              "         1010.,  2009.,  3849.,  2157.,  2005.,  5416., 16189.,  2000.,  4608.,\n",
              "         1011.,  2039.,  2007.,  2060.,  2062.,  3935., 25416., 13490., 14279.,\n",
              "         1010.,  1524.,  6187.,  2226.,  2056.,  1010.,  5815.,  2008.,  2430.,\n",
              "         5085.,  3961.,  1523.,  7933.,  2006.,  2907.,  1524.,  2445.,  1996.,\n",
              "         5703.,  1997., 10831.,  1012.,  2002.,  5275.,  2008.,  1996.,  9561.,\n",
              "         7406., 10750.,  7774.,  2003.,  1523.,  5171.,  2012.,  1996.,  2220.,\n",
              "         5711.,  1997.,  1996.,  5402.,  1010.,  1524.,  1998.,  2008.,  2061.,\n",
              "         2146.,  2004., 17404.,  4897., 12166.,  2024.,  3144.,  1010.,  3930.,\n",
              "         4247.,  2000., 16356., 10745.,  1998.,  2430.,  5085.,  3961., 17145.,\n",
              "         1010., 25416., 13490.,  5649.,  5829.,  2408., 11412.,  4280.,  2298.,\n",
              "         1523., 15123.,  1524.,  1998.,  1041., 15549.,  7368.,  2323.,  2022.,\n",
              "         2583.,  2000., 19319.,  3020.,  6165.,  1012.,  1523.,  1997.,  2607.,\n",
              "         1010.,  2044.,  1996.,  2844.,  2693.,  1997.,  1996.,  2197.,  2261.,\n",
              "         3134.,  1010.,  1041., 15549.,  7368.,  2071.,  2928.,  1037.,  8724.,\n",
              "         2004.,  2116., 11105.,  2008.,  2031., 24356.,  2007., 16189.,  2298.,\n",
              "         2058.,  5092., 18533.,  1010.,  2066., 21955.,  1998.,  5085.,  1010.,\n",
              "         1524.,  6187.,  2226.,  2056.,  1012.,  1523.,  2021.,  2012.,  2023.,\n",
              "         2754.,  1010.,  2057.,  2228.,  4803., 16189.,  2024.,  2062.,  1037.,\n",
              "        13964.,  1997.,  1996., 10067.,  7087.,  3006.,  2084.,  1037.,  5081.,\n",
              "         1010.,  2061., 16510.,  2015.,  2323.,  3613.,  2000.,  2022.,  4149.,\n",
              "         1012.,  1524.,   102.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
              "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# define target chunksize\n",
        "chunksize = 512\n",
        "\n",
        "# split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)\n",
        "input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
        "mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
        "\n",
        "# loop through each chunk\n",
        "for i in range(len(input_id_chunks)):\n",
        "    # add CLS and SEP tokens to input IDs\n",
        "    input_id_chunks[i] = torch.cat([\n",
        "        torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
        "    ])\n",
        "    # add attention tokens to attention mask\n",
        "    mask_chunks[i] = torch.cat([\n",
        "        torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n",
        "    ])\n",
        "    # get required padding length\n",
        "    pad_len = chunksize - input_id_chunks[i].shape[0]\n",
        "    # check if tensor length satisfies required chunk size\n",
        "    if pad_len > 0:\n",
        "        # if padding length is more than 0, we must add padding\n",
        "        input_id_chunks[i] = torch.cat([\n",
        "            input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
        "        ])\n",
        "        mask_chunks[i] = torch.cat([\n",
        "            mask_chunks[i], torch.Tensor([0] * pad_len)\n",
        "        ])\n",
        "\n",
        "# check length of each tensor\n",
        "for chunk in input_id_chunks:\n",
        "    print(len(chunk))\n",
        "# print final chunk so we can see 101, 102, and 0 (PAD) tokens are all correctly placed\n",
        "chunk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long text Preprocessing steps\n",
        "\n",
        "----------------------\n",
        "##1. Defining Chunk Size:\n",
        "\n",
        "    chunksize = 512\n",
        "\n",
        "This line simply sets the desired size for each chunk of text to 512 tokens.\n",
        "\n",
        "------------------\n",
        "##2. Splitting into Chunks:\n",
        "\n",
        "Split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)\n",
        "\n",
        "    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))\n",
        "    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))\n",
        "\n",
        "###tokens['input_ids'][0]\n",
        "\n",
        "and tokens['attention_mask'][0] access the input IDs and attention masks, respectively, from the tokens dictionary.\n",
        "\n",
        "###.split(chunksize - 2)\n",
        "\n",
        "splits these into chunks of chunksize - 2 (which is 510) tokens each.\n",
        "\n",
        "We subtract 2 to make space for the special CLS and SEP tokens that will be added later.\n",
        "\n",
        "###list(...)\n",
        "\n",
        "converts the result (which is a tuple by default) into a list, as lists are mutable (can be modified).\n",
        "\n",
        "------------------------\n",
        "##3. Processing Each Chunk:\n",
        "\n",
        "\n",
        "    for i in range(len(input_id_chunks)):\n",
        "        # ... (code inside the loop)\n",
        "\n",
        "This loop iterates through each chunk in input_id_chunks (and correspondingly in mask_chunks).\n",
        "\n",
        "-------------------\n",
        "\n",
        "##4. Adding Special Tokens:\n",
        "\n",
        "\n",
        "    input_id_chunks[i] = torch.cat([\n",
        "           torch.tensor([101]), input_id_chunks[i], torch.tensor([102])\n",
        "         ])\n",
        "    mask_chunks[i] = torch.cat([\n",
        "        torch.tensor([1]), mask_chunks[i], torch.tensor([1])\n",
        "    ])\n",
        "\n",
        "    torch.cat(...)\n",
        "\n",
        "concatenates (joins) tensors together.\n",
        "\n",
        "For each chunk, it adds the CLS token (101) at the beginning and the SEP token (102) at the end of the input IDs.\n",
        "\n",
        "It also adds corresponding attention mask values (1) for these special tokens.\n",
        "\n",
        "-------------------\n",
        "##5. Padding:\n",
        "\n",
        "\n",
        "    pad_len = chunksize -  \n",
        "       input_id_chunks\n",
        "        [i].shape[0]\n",
        "    if pad_len > 0:\n",
        "        input_id_chunks[i] = torch.cat([\n",
        "            input_id_chunks[i], torch.Tensor([0] * pad_len)\n",
        "        ])\n",
        "        mask_chunks[i] = torch.cat([\n",
        "            mask_chunks[i], torch.Tensor([0] * pad_len)\n",
        "        ])\n",
        "\n",
        "    pad_len\n",
        "\n",
        "calculates the required padding length to make the chunk reach the target size (chunksize).\n",
        "\n",
        "If pad_len is greater than 0 (meaning padding is needed), it adds padding tokens (0) to both the input IDs and attention masks to reach the desired length of 512.\n",
        "\n",
        "-------------------\n",
        "##6. Verification:\n",
        "\n",
        "\n",
        "    for chunk in input_id_chunks:\n",
        "       print(len(chunk))\n",
        "       chunk\n",
        "\n",
        "This loop prints the length of each chunk to verify they are all 512 tokens long after padding.\n",
        "\n",
        "The final chunk is printed to show the structure, including the special tokens and padding.\n",
        "\n",
        "-----------------------\n",
        "#In summary:\n",
        "\n",
        "This code takes a long text, splits it into smaller chunks of 510 tokens, adds special tokens (CLS and SEP) to each chunk, pads the chunks to reach a uniform length of 512 tokens, and then verifies the lengths.\n",
        "\n",
        "\n",
        "This is a common preprocessing step for feeding text data into transformer models like BERT."
      ],
      "metadata": {
        "id": "NO4RjWRpCqrQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHKuMdzbuPw5"
      },
      "source": [
        "It all looks good! Now the final step of placing our tensors back into the dictionary style format we had before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy5_oKqduPw5",
        "outputId": "030b2a58-7f34-4cd6-94dd-e52fb322044e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1045,  2052,  ...,  1012,  1523,   102],\n",
              "         [  101,  1996, 16408,  ...,  2272,  1012,   102],\n",
              "         [  101,  2153,  1010,  ...,     0,     0,     0]]),\n",
              " 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 1, 1, 1],\n",
              "         [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_ids = torch.stack(input_id_chunks)\n",
        "attention_mask = torch.stack(mask_chunks)\n",
        "\n",
        "input_dict = {\n",
        "    'input_ids': input_ids.long(),\n",
        "    'attention_mask': attention_mask.int()\n",
        "}\n",
        "input_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhNg466_uPw6"
      },
      "source": [
        "We can now process all chunks and calculate probabilities using softmax in parallel like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyKSaj7kuPw6",
        "outputId": "0c59310e-6b5a-402c-95b6-a890b01577a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.4144, 0.4940, 0.0916], grad_fn=<MeanBackward1>)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**input_dict)\n",
        "probs = torch.nn.functional.softmax(outputs[0], dim=-1)\n",
        "probs = probs.mean(dim=0)\n",
        "probs"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "ml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}